{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tkinter import Tk, filedialog, messagebox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_video_files() -> list:\n",
    "    \"\"\"Select video files using a file dialog.\n",
    "\n",
    "    Returns:\n",
    "        list: List of selected video files.\n",
    "    \"\"\"\n",
    "    # Initialize Tkinter and hide the root window\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "    \n",
    "    # Open file dialog to select video files\n",
    "    video_files = filedialog.askopenfilenames(\n",
    "        title=\"Select Video Files\",\n",
    "        filetypes=[(\"Video Files\", \"*.mp4 *.avi *.mkv *.mov\")]\n",
    "    )\n",
    "    if not video_files:\n",
    "        raise ValueError(\"No video files selected.\")\n",
    "    \n",
    "    print(f\"Selected {len(video_files)} videos.\")\n",
    "\n",
    "    return video_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_frames(video_files: list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Merge the first frame of each video file into a single image.\n",
    "\n",
    "    Args:\n",
    "        video_files (list): List of video files.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Merged image.\n",
    "    \"\"\"\n",
    "    merged_image = None\n",
    "    \n",
    "    if len(video_files) > 1:\n",
    "        for video_file in video_files:\n",
    "            cap = cv2.VideoCapture(video_file)\n",
    "            success, frame = cap.read()\n",
    "            cap.release()\n",
    "            \n",
    "            if not success:\n",
    "                print(f\"Could not read first frame of {video_file}\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate transparency\n",
    "            transparency = round(1 / len(video_files), 4)\n",
    "            transparent_frame = (frame * transparency).astype(np.uint8)\n",
    "            \n",
    "            if merged_image is None:\n",
    "                # Initialize merged image\n",
    "                merged_image = np.zeros_like(transparent_frame)\n",
    "            \n",
    "            # Add transparent frame to the merged image\n",
    "            merged_image = cv2.add(merged_image, transparent_frame)\n",
    "    \n",
    "    else:\n",
    "        video_file = video_files[0]\n",
    "        cap = cv2.VideoCapture(video_file)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        selected_frame_indices = [1, total_frames//2, total_frames-1] # merge the first, middle, and last frames\n",
    "\n",
    "        for frame_idx in selected_frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "            success, frame = cap.read()\n",
    "\n",
    "            if not success:\n",
    "                print(f\"Could not read frame {frame_idx} from {video_file}\")\n",
    "                continue\n",
    "\n",
    "            # Calculate transparency\n",
    "            transparency = 1/3 # set transparency to 1/3 for each of the three frames\n",
    "            transparent_frame = (frame * transparency).astype(np.uint8)\n",
    "            \n",
    "            if merged_image is None:\n",
    "                # Initialize merged image\n",
    "                merged_image = np.zeros_like(transparent_frame)\n",
    "            \n",
    "            # Add transparent frame to the merged image\n",
    "            merged_image = cv2.add(merged_image, transparent_frame)\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "    return merged_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_points(point_pairs, horizontal=False):\n",
    "\n",
    "    mean_points = np.mean(point_pairs, axis=0)\n",
    "    mean_point_1, mean_point_2 = mean_points.astype(int)\n",
    "\n",
    "    if horizontal:\n",
    "        # Calculate the mean y-value\n",
    "        y_mean = (mean_point_1[1] + mean_point_2[1]) // 2  # Use integer division if you want the result as int\n",
    "\n",
    "        # Update the y-values of both points\n",
    "        mean_point_1[1] = y_mean\n",
    "        mean_point_2[1] = y_mean\n",
    "\n",
    "    print(f\"Mean points: {mean_point_1, mean_point_2}\")\n",
    "    return mean_point_1, mean_point_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_video(video_path, output_folder, rotate_matrix, translate_matrix, width, height, fps): \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_name = os.path.basename(video_path)\n",
    "    output_path = os.path.join(output_folder, video_name)\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Apply transformations\n",
    "        frame = cv2.warpAffine(frame, rotate_matrix, (width, height))\n",
    "        frame = cv2.warpAffine(frame, translate_matrix, (width, height))\n",
    "        out.write(frame)\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f'Aligned {video_name}')\n",
    "\n",
    "def apply_transformations(video_files, point_pairs, mean_point_1, mean_point_2):\n",
    "    output_folder = os.path.join(os.path.dirname(video_files[0]), 'aligned')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Compute global mean properties\n",
    "    mean_vector = mean_point_2 - mean_point_1\n",
    "    mean_length = np.linalg.norm(mean_vector)\n",
    "    mean_angle = np.arctan2(mean_vector[1], mean_vector[0])\n",
    "\n",
    "    for video_path, (point1, point2) in zip(video_files, point_pairs):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        cap.release()\n",
    "\n",
    "        # Compute transformation parameters\n",
    "        vector = np.array(point2) - np.array(point1)\n",
    "        length = np.linalg.norm(vector)\n",
    "        angle = np.arctan2(vector[1], vector[0])\n",
    "\n",
    "        if length == 0:  # Avoid division by zero\n",
    "            print(f\"Skipping {video_path} due to zero-length vector.\")\n",
    "            continue\n",
    "\n",
    "        scale = mean_length / length\n",
    "        rotation_angle = np.degrees(mean_angle - angle)\n",
    "\n",
    "        # Rotation matrix\n",
    "        center = (width // 2, height // 2)\n",
    "        rotate_matrix = cv2.getRotationMatrix2D(center, rotation_angle, scale)\n",
    "\n",
    "        # Compute translation matrix\n",
    "        new_point1 = rotate_matrix[:, :2] @ np.array(point1).T + rotate_matrix[:, 2]\n",
    "        dx, dy = mean_point_1 - new_point1\n",
    "        translate_matrix = np.float32([[1, 0, dx], [0, 1, dy]])\n",
    "\n",
    "        save_video(video_path, output_folder, rotate_matrix, translate_matrix, width, height, fps)\n",
    "        print(f\"Scale {scale:.2f}, Rotation {rotation_angle:.2f}Â°, Translation {dx:.2f}, {dy:.2f}\")\n",
    "\n",
    "    print(f\"Aligned videos saved in '{output_folder}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom_in_display(frame, x, y, zoom_scale = 5, zoom_window_size = 25):\n",
    "    # Create zoomed-in display\n",
    "    x1 = max(0, x - zoom_window_size)\n",
    "    x2 = min(frame.shape[1], x + zoom_window_size)\n",
    "    y1 = max(0, y - zoom_window_size)\n",
    "    y2 = min(frame.shape[0], y + zoom_window_size)\n",
    "\n",
    "    zoomed_area = frame[y1:y2, x1:x2]\n",
    "    \n",
    "    # Resize zoomed-in area\n",
    "    zoomed_area_resized = cv2.resize(zoomed_area, None, fx=zoom_scale, fy=zoom_scale, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # Add crosshair to the center\n",
    "    center_x = zoomed_area_resized.shape[1] // 2\n",
    "    center_y = zoomed_area_resized.shape[0] // 2\n",
    "    color = (0, 255, 0)  # Black crosshair\n",
    "    thickness = 2\n",
    "    line_length = 20  # Length of crosshair lines\n",
    "\n",
    "    # Draw vertical line\n",
    "    cv2.line(zoomed_area_resized, (center_x, center_y - line_length), (center_x, center_y + line_length), color, thickness)\n",
    "    # Draw horizontal line\n",
    "    cv2.line(zoomed_area_resized, (center_x - line_length, center_y), (center_x + line_length, center_y), color, thickness)\n",
    "\n",
    "    if x2 > (frame.shape[1] - zoomed_area_resized.shape[1] - 10) and y1 < (10 + zoomed_area_resized.shape[0]):\n",
    "        # Overlay zoomed-in area in the top-left corner of the frame\n",
    "        overlay_x1 = 10\n",
    "        overlay_x2 = 10 + zoomed_area_resized.shape[1]\n",
    "        overlay_y1 = 10\n",
    "        overlay_y2 = 10 + zoomed_area_resized.shape[0]\n",
    "    \n",
    "    else:\n",
    "        # Overlay zoomed-in area in the top-right corner of the frame\n",
    "        overlay_x1 = frame.shape[1] - zoomed_area_resized.shape[1] - 10\n",
    "        overlay_x2 = frame.shape[1] - 10\n",
    "        overlay_y1 = 10\n",
    "        overlay_y2 = 10 + zoomed_area_resized.shape[0]\n",
    "\n",
    "    placement = (overlay_x1, overlay_x2, overlay_y1, overlay_y2)\n",
    "\n",
    "    return zoomed_area_resized, placement\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_point_pairs(video_files: list) -> list:\n",
    "\n",
    "    print(\"Instructions:\")\n",
    "    print(\"1. Left-click to select points.\")\n",
    "    print(\"2. Enter to confirm the current point.\")\n",
    "    print(\"3. Select two points on each video to align them.\")\n",
    "    print(\"Press 'q' to quit without aligning.\")\n",
    "\n",
    "    # Initialize Tkinter and hide the root window\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "    \n",
    "    # Initialize variables\n",
    "    point_pairs = []  # To store pairs of points for each video\n",
    "    first_frames = []\n",
    "\n",
    "    # Define callback function for point selection\n",
    "    def select_points(event, x, y, flags, param):\n",
    "        nonlocal frame, temp_frame, current_point, confirmed_points\n",
    "\n",
    "        #if event == cv2.EVENT_MOUSEMOVE:   \n",
    "\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            # Update the current point with the clicked position\n",
    "            current_point = (x, y)\n",
    "            # Draw the current point\n",
    "            cv2.circle(temp_frame, current_point, 3, (0, 255, 0), -1)\n",
    "            # Draw the confirmed points on the frame\n",
    "            for point in confirmed_points: \n",
    "                cv2.circle(temp_frame, point, 3, (0, 0, 255), -1)\n",
    "            # Display the frame\n",
    "            cv2.imshow('Select Points', temp_frame)\n",
    "        \n",
    "        # Reset the frame\n",
    "        temp_frame = frame.copy()\n",
    "\n",
    "        # Draw the current point\n",
    "        if current_point is not None:\n",
    "            cv2.circle(temp_frame, current_point, 3, (0, 255, 0), -1)\n",
    "        # Draw the confirmed points on the frame\n",
    "        for point in confirmed_points:\n",
    "            cv2.circle(temp_frame, point, 3, (0, 0, 255), -1)\n",
    "        # Display the zoomed-in area\n",
    "        zoomed_area_resized, placement = zoom_in_display(temp_frame, x, y)\n",
    "        overlay_x1, overlay_x2, overlay_y1, overlay_y2 = placement\n",
    "        temp_frame[overlay_y1:overlay_y2, overlay_x1:overlay_x2] = zoomed_area_resized\n",
    "        # Display the frame\n",
    "        cv2.imshow('Select Points', temp_frame)\n",
    "\n",
    "    def confirm_point():\n",
    "        \"\"\"Confirm the current point and add it to the list.\"\"\"\n",
    "        nonlocal temp_frame, confirmed_points, current_point\n",
    "        if current_point is not None:\n",
    "            confirmed_points.append(current_point)\n",
    "            # Draw the confirmed points on the frame\n",
    "            for point in confirmed_points: \n",
    "                cv2.circle(temp_frame, point, 3, (0, 0, 255), -1)\n",
    "            # Display the frame\n",
    "            cv2.imshow('Select Points', temp_frame)\n",
    "            current_point = None\n",
    "            print(f\"Point confirmed: {confirmed_points[-1]}\")  # Feedback to the user\n",
    "    \n",
    "    # Step 1: Extract first frames and collect two points for each video\n",
    "    for video_path in video_files:\n",
    "        frame = merge_frames([video_path]) # we make video_path a list because merge_frames expects a list\n",
    "        first_frames.append((frame, video_path))\n",
    "        confirmed_points = []  # Store the two confirmed points for this video\n",
    "        current_point = None  # Temporary point being adjusted\n",
    "        temp_frame = frame.copy()  # Create a copy of the frame\n",
    "\n",
    "        # Run the mouse callback with the frame and confirmed points\n",
    "        cv2.imshow('Select Points', frame)\n",
    "        cv2.setMouseCallback('Select Points', select_points)\n",
    "\n",
    "        # Wait for user to confirm two points\n",
    "        while len(confirmed_points) < 2:\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == 13:  # Enter key to confirm the current point\n",
    "                confirm_point()\n",
    "            elif key == ord('q'):  # Press 'q' to quit\n",
    "                response = messagebox.askquestion(\"Exit\", \"Do you want to exit aligner?\")\n",
    "                if response == 'yes':\n",
    "                    print(\"Exiting point selection.\")\n",
    "                    cv2.destroyAllWindows()\n",
    "                    return\n",
    "            \n",
    "        # Save the confirmed points\n",
    "        point_pairs.append(confirmed_points)\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return point_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_videos():\n",
    "    # Open file dialog to select video files\n",
    "    video_files = select_video_files()\n",
    "\n",
    "    point_pairs = select_point_pairs(video_files)\n",
    "\n",
    "    response = messagebox.askquestion(\"Alignment\", \"Do you want the points to stand on the same horizontal line?\")  \n",
    "    mean_point_1, mean_point_2 = calculate_mean_points(point_pairs, horizontal = response)\n",
    "\n",
    "    apply_transformations(video_files, point_pairs, mean_point_1, mean_point_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 2 videos.\n",
      "Instructions:\n",
      "1. Left-click to select points.\n",
      "2. Enter to confirm the current point.\n",
      "3. Select two points on each video to align them.\n",
      "Press 'q' to quit without aligning.\n",
      "Point confirmed: (138, 111)\n",
      "Point confirmed: (840, 111)\n",
      "Point confirmed: (138, 110)\n",
      "Point confirmed: (840, 110)\n",
      "Mean points: (array([138, 110]), array([840, 110]))\n",
      "Aligned social_R01_Hab.avi\n",
      "Scale 1.00, Rotation 0.00Â°, Translation 0.00, -1.00\n",
      "Aligned social_R01_TR1.avi\n",
      "Scale 1.00, Rotation 0.00Â°, Translation 0.00, 0.00\n",
      "Aligned videos saved in 'C:/Users/dhers/Desktop/prueba\\aligned'.\n"
     ]
    }
   ],
   "source": [
    "align_videos()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rainstorm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
