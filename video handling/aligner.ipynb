{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAIN - Real & Artificial Intelligence in Neuroscience\n",
    "\n",
    "## Video aligner\n",
    "\n",
    "How to use:\n",
    "- Select the videos you want to align\n",
    "- On each frame, click on two points that you want to be aligned between videos\n",
    "- The aligned videos will be stored in the 'Aligned' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tkinter import Tk, filedialog, messagebox\n",
    "\n",
    "import random\n",
    "\n",
    "def merge_random_frames(video_file, num_frames: int = 5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Merge a specified number of random frames from each video file into a single image.\n",
    "\n",
    "    Args:\n",
    "        num_frames (int): Number of random frames to merge from each video. Default is 5.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Merged image.\n",
    "    \"\"\"\n",
    "    merged_image = None\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    selected_frame_indices = random.sample(range(total_frames), num_frames)\n",
    "\n",
    "    for frame_idx in selected_frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        success, frame = cap.read()\n",
    "\n",
    "        if not success:\n",
    "            print(f\"Could not read frame {frame_idx} from {video_file}\")\n",
    "            continue\n",
    "\n",
    "        # Calculate transparency\n",
    "        transparency = round(1 / num_frames, 4)\n",
    "        transparent_frame = (frame * transparency).astype(np.uint8)\n",
    "        \n",
    "        if merged_image is None:\n",
    "            # Initialize merged image\n",
    "            merged_image = np.zeros_like(transparent_frame)\n",
    "        \n",
    "        # Add transparent frame to the merged image\n",
    "        merged_image = cv2.add(merged_image, transparent_frame)\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    return merged_image\n",
    "\n",
    "def align_videos():\n",
    "\n",
    "    print(\"Instructions:\")\n",
    "    print(\"1. Left-click to select points.\")\n",
    "    print(\"2. Enter to confirm the current point.\")\n",
    "    print(\"3. Select two points on each video to align them.\")\n",
    "    print(\"Press 'q' to quit without aligning.\")\n",
    "\n",
    "    # Initialize Tkinter and hide the root window\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "    \n",
    "    # Open file dialog to select video files\n",
    "    video_files = filedialog.askopenfilenames(\n",
    "        title=\"Select Video Files\",\n",
    "        filetypes=[(\"Video Files\", \"*.mp4 *.avi *.mkv *.mov\")]\n",
    "    )\n",
    "    if not video_files:\n",
    "        raise ValueError(\"No video files selected.\")\n",
    "    \n",
    "    print(f\"Selected {len(video_files)} videos.\")\n",
    "\n",
    "    # Initialize variables\n",
    "    zoom_scale = 5  # How much to zoom in\n",
    "    zoom_window_size = 25  # Half the width/height of the zoomed-in area\n",
    "    point_pairs = []  # To store pairs of points for each video\n",
    "    first_frames = []\n",
    "\n",
    "    # Define callback function for point selection\n",
    "    def select_points(event, x, y, flags, param):\n",
    "        nonlocal frame, temp_frame, current_point, confirmed_points, zoom_scale, zoom_window_size\n",
    "\n",
    "        #if event == cv2.EVENT_MOUSEMOVE:   \n",
    "\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            # Update the current point with the clicked position\n",
    "            current_point = (x, y)\n",
    "            # Draw the current point\n",
    "            cv2.circle(temp_frame, current_point, 3, (0, 255, 0), -1)\n",
    "            # Draw the confirmed points on the frame\n",
    "            for point in confirmed_points: \n",
    "                cv2.circle(temp_frame, point, 3, (0, 0, 255), -1)\n",
    "            # Display the frame\n",
    "            cv2.imshow('Select Points', temp_frame)\n",
    "        \n",
    "        # Create zoomed-in display\n",
    "        x1 = max(0, x - zoom_window_size)\n",
    "        x2 = min(temp_frame.shape[1], x + zoom_window_size)\n",
    "        y1 = max(0, y - zoom_window_size)\n",
    "        y2 = min(temp_frame.shape[0], y + zoom_window_size)\n",
    "\n",
    "        zoomed_area = temp_frame[y1:y2, x1:x2]\n",
    "        \n",
    "        # Resize zoomed-in area\n",
    "        zoomed_area_resized = cv2.resize(zoomed_area, None, fx=zoom_scale, fy=zoom_scale, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        # Add crosshair to the center\n",
    "        center_x = zoomed_area_resized.shape[1] // 2\n",
    "        center_y = zoomed_area_resized.shape[0] // 2\n",
    "        color = (0, 255, 0)  # Black crosshair\n",
    "        thickness = 2\n",
    "        line_length = 20  # Length of crosshair lines\n",
    "\n",
    "        # Draw vertical line\n",
    "        cv2.line(zoomed_area_resized, (center_x, center_y - line_length), (center_x, center_y + line_length), color, thickness)\n",
    "        # Draw horizontal line\n",
    "        cv2.line(zoomed_area_resized, (center_x - line_length, center_y), (center_x + line_length, center_y), color, thickness)\n",
    "\n",
    "        if x2 > (temp_frame.shape[1] - zoomed_area_resized.shape[1] - 10) and y1 < (10 + zoomed_area_resized.shape[0]):\n",
    "            # Overlay zoomed-in area in the top-left corner of the frame\n",
    "            overlay_x1 = 10\n",
    "            overlay_x2 = 10 + zoomed_area_resized.shape[1]\n",
    "            overlay_y1 = 10\n",
    "            overlay_y2 = 10 + zoomed_area_resized.shape[0]\n",
    "        \n",
    "        else:\n",
    "            # Overlay zoomed-in area in the top-right corner of the frame\n",
    "            overlay_x1 = temp_frame.shape[1] - zoomed_area_resized.shape[1] - 10\n",
    "            overlay_x2 = temp_frame.shape[1] - 10\n",
    "            overlay_y1 = 10\n",
    "            overlay_y2 = 10 + zoomed_area_resized.shape[0]\n",
    "        \n",
    "        # Reset the frame\n",
    "        temp_frame = frame.copy()\n",
    "\n",
    "        # Draw the current point\n",
    "        if current_point is not None:\n",
    "            cv2.circle(temp_frame, current_point, 3, (0, 255, 0), -1)\n",
    "        # Draw the confirmed points on the frame\n",
    "        for point in confirmed_points:\n",
    "            cv2.circle(temp_frame, point, 3, (0, 0, 255), -1)\n",
    "        # Display the zoomed-in area\n",
    "        temp_frame[overlay_y1:overlay_y2, overlay_x1:overlay_x2] = zoomed_area_resized\n",
    "        # Display the frame\n",
    "        cv2.imshow('Select Points', temp_frame)\n",
    "\n",
    "    def confirm_point():\n",
    "        \"\"\"Confirm the current point and add it to the list.\"\"\"\n",
    "        nonlocal temp_frame, confirmed_points, current_point\n",
    "        if current_point is not None:\n",
    "            confirmed_points.append(current_point)\n",
    "            # Draw the confirmed points on the frame\n",
    "            for point in confirmed_points: \n",
    "                cv2.circle(temp_frame, point, 3, (0, 0, 255), -1)\n",
    "            # Display the frame\n",
    "            cv2.imshow('Select Points', temp_frame)\n",
    "            current_point = None\n",
    "            print(f\"Point confirmed: {confirmed_points[-1]}\")  # Feedback to the user\n",
    "    \n",
    "    # Step 1: Extract first frames and collect two points for each video\n",
    "    for video_path in video_files:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frame = merge_random_frames(video_path)\n",
    "        first_frames.append((frame, video_path))\n",
    "        confirmed_points = []  # Store the two confirmed points for this video\n",
    "        current_point = None  # Temporary point being adjusted\n",
    "        temp_frame = frame.copy()  # Create a copy of the frame\n",
    "\n",
    "        # Run the mouse callback with the frame and confirmed points\n",
    "        cv2.imshow('Select Points', frame)\n",
    "        cv2.setMouseCallback('Select Points', select_points)\n",
    "\n",
    "        # Wait for user to confirm two points\n",
    "        while len(confirmed_points) < 2:\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == 13:  # Enter key to confirm the current point\n",
    "                confirm_point()\n",
    "            elif key == ord('q'):  # Press 'q' to quit\n",
    "                response = messagebox.askquestion(\"Exit\", \"Do you want to exit aligner?\")\n",
    "                if response == 'yes':\n",
    "                    print(\"Exiting point selection.\")\n",
    "                    cv2.destroyAllWindows()\n",
    "                    return\n",
    "            \n",
    "        # Save the confirmed points\n",
    "        point_pairs.append(confirmed_points)\n",
    "        cap.release()\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Step 2: Calculate mean points\n",
    "    if not point_pairs:\n",
    "        print(\"No points were selected.\")\n",
    "        return\n",
    "    \n",
    "    mean_points = np.mean(point_pairs, axis=0)\n",
    "    mean_point1, mean_point2 = mean_points.astype(int)\n",
    "\n",
    "    response = messagebox.askquestion(\"Alignment\", \"Do you want the points to stand on the same horizontal line?\")  \n",
    "    if response == 'yes':\n",
    "        # Calculate the mean y-value\n",
    "        y_mean = (mean_point1[1] + mean_point2[1]) // 2  # Use integer division if you want the result as int\n",
    "\n",
    "        # Update the y-values of both points\n",
    "        mean_point1[1] = y_mean\n",
    "        mean_point2[1] = y_mean\n",
    "    \n",
    "    print(f\"Mean points: {mean_point1}, {mean_point2}\")\n",
    "    \n",
    "    # Step 3: Align videos (rotate, resize, then translate)\n",
    "    output_folder = os.path.join(os.path.dirname(video_files[0]), 'Aligned')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    mean_vector = mean_point2 - mean_point1\n",
    "    mean_length = np.linalg.norm(mean_vector)\n",
    "    mean_angle = np.arctan2(mean_vector[1], mean_vector[0])\n",
    "    \n",
    "    for (frame, video_path), points in zip(first_frames, point_pairs):\n",
    "        point1, point2 = points\n",
    "        vector = np.array(point2) - np.array(point1)\n",
    "        angle = np.arctan2(vector[1], vector[0])\n",
    "        length = np.linalg.norm(vector)\n",
    "        \n",
    "        scale = mean_length / length\n",
    "        rotation_angle = mean_angle + angle\n",
    "\n",
    "        # Step 3.1: Rotate and resize\n",
    "        height, width = frame.shape[:2]\n",
    "        center = (width // 2, height // 2)\n",
    "        M_rotate_scale = cv2.getRotationMatrix2D(center, np.degrees(rotation_angle), scale)\n",
    "        rotated_resized_frame = cv2.warpAffine(frame, M_rotate_scale, (width, height))\n",
    "        \n",
    "        # Step 3.2: Translate\n",
    "        new_point1 = np.dot(M_rotate_scale[:, :2], np.array(point1).T) + M_rotate_scale[:, 2]\n",
    "        dx, dy = mean_point1[0] - new_point1[0], mean_point1[1] - new_point1[1]\n",
    "        M_translate = np.float32([[1, 0, dx], [0, 1, dy]])\n",
    "        aligned_frame = cv2.warpAffine(rotated_resized_frame, M_translate, frame.shape[1::-1])\n",
    "        \n",
    "        # Save aligned video\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        video_name = os.path.basename(video_path)\n",
    "        output_path = os.path.join(output_folder, video_name.replace('.', '_aligned.'))\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Apply the same transformations to each frame\n",
    "            rotated_resized_frame = cv2.warpAffine(frame, M_rotate_scale, frame.shape[1::-1])\n",
    "            aligned_frame = cv2.warpAffine(rotated_resized_frame, M_translate, frame.shape[1::-1])\n",
    "            out.write(aligned_frame)\n",
    "        \n",
    "        cap.release()\n",
    "        out.release()\n",
    "\n",
    "        print(f\"Aligned '{video_name}' with scale {scale:.2f}, rotation {rotation_angle:.2f}, and translation {dx:.2f}, {dy:.2f}.\")\n",
    "    \n",
    "    print(f\"Aligned videos saved in '{output_folder}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instructions:\n",
      "1. Left-click to select points.\n",
      "2. Enter to confirm the current point.\n",
      "3. Select two points on each video to align them.\n",
      "Press 'q' to quit without aligning.\n",
      "Selected 18 videos.\n",
      "Point confirmed: (363, 398)\n",
      "Point confirmed: (1248, 403)\n",
      "Point confirmed: (368, 397)\n",
      "Point confirmed: (1252, 400)\n",
      "Point confirmed: (367, 395)\n",
      "Point confirmed: (1252, 400)\n",
      "Point confirmed: (357, 419)\n",
      "Point confirmed: (1243, 406)\n",
      "Point confirmed: (359, 416)\n",
      "Point confirmed: (1246, 403)\n",
      "Point confirmed: (360, 416)\n",
      "Point confirmed: (1246, 403)\n",
      "Point confirmed: (361, 416)\n",
      "Point confirmed: (1246, 403)\n",
      "Point confirmed: (359, 415)\n",
      "Point confirmed: (1246, 402)\n",
      "Point confirmed: (359, 413)\n",
      "Point confirmed: (1244, 402)\n",
      "Point confirmed: (358, 413)\n",
      "Point confirmed: (1244, 402)\n",
      "Point confirmed: (359, 414)\n",
      "Point confirmed: (1243, 403)\n",
      "Point confirmed: (357, 413)\n",
      "Point confirmed: (1242, 402)\n",
      "Point confirmed: (357, 414)\n",
      "Point confirmed: (1242, 403)\n",
      "Point confirmed: (357, 414)\n",
      "Point confirmed: (1242, 402)\n",
      "Point confirmed: (356, 414)\n",
      "Point confirmed: (1242, 402)\n",
      "Point confirmed: (357, 414)\n",
      "Point confirmed: (1243, 402)\n",
      "Point confirmed: (357, 415)\n",
      "Point confirmed: (1243, 403)\n",
      "Point confirmed: (356, 414)\n",
      "Point confirmed: (1243, 403)\n",
      "Mean points: [359 406], [1244  406]\n",
      "Aligned 'WIN_20240405_13_49_18_Pro.mp4' with scale 1.00, rotation 0.01, and translation -3.72, 5.53.\n",
      "Aligned 'WIN_20240405_13_56_38_Pro.mp4' with scale 1.00, rotation 0.00, and translation -8.34, 7.59.\n",
      "Aligned 'WIN_20240405_14_04_19_Pro.mp4' with scale 1.00, rotation 0.01, and translation -7.70, 8.55.\n",
      "Aligned 'WIN_20240405_14_12_24_Pro.mp4' with scale 1.00, rotation -0.01, and translation 0.95, -6.55.\n",
      "Aligned 'WIN_20240405_14_19_42_Pro.mp4' with scale 1.00, rotation -0.01, and translation -1.59, -3.64.\n",
      "Aligned 'WIN_20240405_14_26_38_Pro.mp4' with scale 1.00, rotation -0.01, and translation -2.09, -3.60.\n",
      "Aligned 'WIN_20240405_14_34_05_Pro.mp4' with scale 1.00, rotation -0.01, and translation -2.59, -3.56.\n",
      "Aligned 'WIN_20240405_14_41_30_Pro.mp4' with scale 1.00, rotation -0.01, and translation -1.60, -2.64.\n",
      "Aligned 'WIN_20240405_14_50_19_Pro.mp4' with scale 1.00, rotation -0.01, and translation -0.53, -1.53.\n",
      "Aligned 'WIN_20240405_14_57_25_Pro.mp4' with scale 1.00, rotation -0.01, and translation -0.03, -1.57.\n",
      "Aligned 'WIN_20240405_15_04_22_Pro.mp4' with scale 1.00, rotation -0.01, and translation -0.02, -2.47.\n",
      "Aligned 'WIN_20240405_15_47_10_Pro.mp4' with scale 1.00, rotation -0.01, and translation 1.47, -1.50.\n",
      "Aligned 'WIN_20240405_15_53_42_Pro.mp4' with scale 1.00, rotation -0.01, and translation 1.48, -2.50.\n",
      "Aligned 'WIN_20240405_16_00_26_Pro.mp4' with scale 1.00, rotation -0.01, and translation 1.43, -2.00.\n",
      "Aligned 'WIN_20240405_16_07_28_Pro.mp4' with scale 1.00, rotation -0.01, and translation 1.93, -2.04.\n",
      "Aligned 'WIN_20240405_16_14_38_Pro.mp4' with scale 1.00, rotation -0.01, and translation 0.93, -2.06.\n",
      "Aligned 'WIN_20240405_16_21_01_Pro.mp4' with scale 1.00, rotation -0.01, and translation 0.95, -3.05.\n",
      "Aligned 'WIN_20240405_16_27_23_Pro.mp4' with scale 1.00, rotation -0.01, and translation 1.49, -2.59.\n",
      "Aligned videos saved in 'C:/Users/dhers/OneDrive/Doctorado/Experimentos/3xTg_B2/2024_04-E+M/resized\\Aligned'.\n"
     ]
    }
   ],
   "source": [
    "align_videos()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rainstorm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
